{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a14a4-4dc1-4e9b-a6e2-7ff9a23627a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenize(text):\n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    stemObj = PorterStemmer()\n",
    "    tokens = [stemObj.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "data_dir = 'Data'\n",
    "\n",
    "#interate \n",
    "for i in range(10):\n",
    "    pathRel = os.path.join(data_dir, f'a{i+1}.txt')\n",
    "    \n",
    "    with open(pathRel, 'r') as file:\n",
    "        txtFile = file.read()\n",
    "\n",
    "    document_tokens = tokenize(txtFile)\n",
    "    totalTk = len(document_tokens)\n",
    "    tks = len(set(document_tokens))\n",
    "\n",
    "    print(f\"total tokens in doc {i+1}: {totalTk}\")\n",
    "    print(f\"(unique tokens) in doc {i+1}: {tks}\")\n",
    "    print()\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15847c5c-3e46-4ec0-b0eb-322f0e836308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    stemObj = PorterStemmer()\n",
    "    tokens = [stemObj.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "data_dir = 'Data'\n",
    "for i in range(10):\n",
    "    pathRel = os.path.join(data_dir, f'a{i+1}.txt') \n",
    "    \n",
    "    with open(pathRel, 'r') as file:\n",
    "        txtFile = file.read()\n",
    "\n",
    "    document_tokens = tokenize_and_remove_stopwords(txtFile)\n",
    "    totalTk = len(document_tokens)\n",
    "    tks = len(set(document_tokens))\n",
    "\n",
    "    print(f\"Total tokens in doc {i+1} after stop-word removd: {totalTk}\")\n",
    "    print(f\"# of types unique tokens in doc {i+1} after stop-word removal: {tks}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1706e-5b00-40a3-8156-d8f1d66aa669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def tknFun(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    stemObj = PorterStemmer()\n",
    "    tokens = [stemObj.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "data_dir = 'Data'\n",
    "\n",
    "corpDict = set()\n",
    "\n",
    "for i in range(10):\n",
    "    pathRel = os.path.join(data_dir, f'a{i+1}.txt')  \n",
    "    \n",
    "    with open(pathRel, 'r') as file:\n",
    "        txtFile = file.read()\n",
    "\n",
    "    document_tokens = tknFun(txtFile)\n",
    "\n",
    "    corpDict.update(document_tokens)\n",
    "    num_terms = len(set(document_tokens))\n",
    "\n",
    "    print(f\"Number of terms in document {i+1} after stemming: {num_terms}\")\n",
    "    print()\n",
    "\n",
    "total_vocabulary_size = len(corpDict)\n",
    "print(f\"Total vocabulary size for the entire corpus after stemming: {total_vocabulary_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa497c1-0f60-4cab-b46a-3f6878f53fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "data_dir = 'Data'\n",
    "corpus = []\n",
    "for i in range(10):\n",
    "    pathRel = os.path.join(data_dir, f'a{i+1}.txt') \n",
    "    \n",
    "    with open(pathRel, 'r') as file:\n",
    "        txtFile = file.read()\n",
    "\n",
    "    corpus.append(txtFile)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "document_names = [f'Document {i+1}' for i in range(10)]\n",
    "\n",
    "import pandas as pd\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=document_names)\n",
    "\n",
    "print(\"TF-IDF features for each document:\")\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195856ee-8f94-47e6-9416-d3037a9f7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 5\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data_dir = 'Data'\n",
    "corpus = []\n",
    "for i in range(10):\n",
    "    pathRel = os.path.join(data_dir, f'a{i+1}.txt')\n",
    "    \n",
    "    with open(pathRel, 'r') as file:\n",
    "        txtFile = file.read()\n",
    "\n",
    "    corpus.append(txtFile)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "print(\"Cosine Matrix:\")\n",
    "print(cosine_sim_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf6797-c064-408c-b97f-c0cf34204bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
